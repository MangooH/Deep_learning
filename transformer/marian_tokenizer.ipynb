{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MarianTokenizer 사용해보기\n",
    "> Transformer 구현에 사용된 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer \n",
    "# MT: Machine Translation\n",
    "\n",
    "# Load the tokenizer & model\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ko-en')\n",
    "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ko-en') # MT: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필수적인 Token\n",
    "- `eos`: 문장의 시작과 끝을 알리는 토큰\n",
    "- `pad`: Batch 의 Size 를 맞춰주기 위해 부족한 길이를 채우는 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_idx = 0\n",
      "pad_idx = 65000\n",
      "Vocabulary size: 65001\n"
     ]
    }
   ],
   "source": [
    "# 필수적인 token 의 ID\n",
    "eos_idx = tokenizer.eos_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "print('eos_idx =', eos_idx)\n",
    "print('pad_idx =', pad_idx)\n",
    "\n",
    "# 토크나이저의 어휘 사전 크기를 가져옴\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"Vocabulary size:\",vocab_size)\n",
    "\n",
    "# 토크나이저의 어휘 사전\n",
    "# print(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토크나이저 & 학습된 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [토크나이저 참고 자료 - Byte Pair Encoding](https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁토', '크', '나이', '저']\n",
      "['▁요새', '는', '▁Cl', 'au', 'de', '▁가', '▁더', '▁좋은', '▁듯']\n",
      "['▁한', '글', '에서', '▁제일', '▁좋은', '▁to', 'k', 'en', 'iz', 'er', '▁는', '▁뭘', '까']\n"
     ]
    }
   ],
   "source": [
    "# _ 는 띄어쓰기를 의미하며, 단어, 음절이 아닌 Subword tokenizing 방식\n",
    "print(tokenizer.tokenize(\"토크나이저\"))\n",
    "print(tokenizer.tokenize(\"요새는 Claude 가 더 좋은 듯\"))\n",
    "print(tokenizer.tokenize(\"한글에서 제일 좋은 tokenizer 는 뭘까\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19025]\n",
      "[34359]\n",
      "[65000]\n",
      "[0]\n",
      "----------\n",
      "['▁문장', '을', '▁넣으면', '▁토', '크', '나이', '즈', '를', '▁하여', '▁숫자', '로', '▁바꾼', '다']\n",
      "[13774, 51, 40068, 4155, 1020, 5037, 1329, 79, 242, 6635, 131, 30737, 161]\n",
      "사람\n",
      "으로\n",
      "make\n",
      "to of? and<pad> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "# word to index\n",
    "print(tokenizer.encode('인공', add_special_tokens=False)) \n",
    "print(tokenizer.encode('지능', add_special_tokens=False)) \n",
    "print(tokenizer.encode('<pad>', add_special_tokens=False)) \n",
    "print(tokenizer.encode('</s>', add_special_tokens=False))\n",
    "print('-' * 10)\n",
    "\n",
    "# Sentence to index\n",
    "print(tokenizer.tokenize(\"문장을 넣으면 토크나이즈를 하여 숫자로 바꾼다\"))\n",
    "print(tokenizer.encode('문장을 넣으면 토크나이즈를 하여 숫자로 바꾼다', add_special_tokens=False)) \n",
    "\n",
    "# index to string\n",
    "print(tokenizer.decode([204]))\n",
    "print(tokenizer.decode([206]))\n",
    "print(tokenizer.decode([210]))\n",
    "print(tokenizer.decode(list(range(5,9)) + [65000,65001,65002,65003]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 토크나이저는 문장을 숫자로 바꾼다\n",
      "AI의 번역: Toknaiser turns sentences into numbers.\n"
     ]
    }
   ],
   "source": [
    "# 사전 학습된 모델로 번역해보기\n",
    "input_text = \"토크나이저는 문장을 숫자로 바꾼다\"\n",
    "\n",
    "input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(input_tokens)\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"입력:\", input_text)\n",
    "print(\"AI의 번역:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset, DataLoader 생성 및 배치단위 토크나이징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [AI hub](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1r4ZnFJOStyBlNRx7snBQ-Iq2GNyJKL6t\n",
      "To: /root/transformer/대화체.xlsx\n",
      "100%|██████████████████████████████████████| 9.57M/9.57M [00:00<00:00, 27.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "!gdown https://drive.google.com/uc?id=1r4ZnFJOStyBlNRx7snBQ-Iq2GNyJKL6t -O '대화체.xlsx'\n",
    "data = pd.read_excel('대화체.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>대분류</th>\n",
       "      <th>소분류</th>\n",
       "      <th>상황</th>\n",
       "      <th>Set Nr.</th>\n",
       "      <th>발화자</th>\n",
       "      <th>원문</th>\n",
       "      <th>번역문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>비즈니스</td>\n",
       "      <td>회의</td>\n",
       "      <td>의견 교환하기</td>\n",
       "      <td>1</td>\n",
       "      <td>A-1</td>\n",
       "      <td>이번 신제품 출시에 대한 시장의 반응은 어떤가요?</td>\n",
       "      <td>How is the market's reaction to the newly rele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    대분류 소분류       상황  Set Nr.  발화자                           원문  \\\n",
       "0  비즈니스  회의  의견 교환하기        1  A-1  이번 신제품 출시에 대한 시장의 반응은 어떤가요?   \n",
       "\n",
       "                                                 번역문  \n",
       "0  How is the market's reaction to the newly rele...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95000\n",
      "3000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data.loc[index, '원문'], self.data.loc[index, '번역문']\n",
    "    \n",
    "custom_DS = CustomDataset(data=data)\n",
    "\n",
    "train_DS, val_DS, test_DS= torch.utils.data.random_split(custom_DS, [95000, 3000, 2000])\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_DL = torch.utils.data.DataLoader(train_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_DL = torch.utils.data.DataLoader(val_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_DL = torch.utils.data.DataLoader(test_DS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_DS))\n",
    "print(len(val_DS))\n",
    "print(len(test_DS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64\n",
      "('그건 진짜 무례한데 어떻게 사과를 안 할 수가 있지?', '입국심사장 가는 길에 제 여권이 없다는 걸 발견했어요. 잃어버린 것 같아요.')\n",
      "('How can someone not apologize for such a rude act?', \"I realized that I don't have my passport on the way to immigration. I think I lost it.\")\n",
      "-----Tokenizing-----\n",
      "torch.Size([64, 27]) torch.Size([64, 48])\n",
      "tensor([[  995,  1670, 28049, 13210,   333,  7980,    79,   245,   239,  3211,\n",
      "          1695,     7,     0, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000],\n",
      "        [ 2341,  1815,   865, 20095,  3222, 17144,   303, 43359,    48,  6956,\n",
      "           637, 45786,     2, 29154,   346,  3300,     2,     0, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000]])\n"
     ]
    }
   ],
   "source": [
    "max_len = 100\n",
    "for src_texts, trg_texts in train_DL:\n",
    "    \n",
    "    print(len(src_texts), len(trg_texts))\n",
    "    print(src_texts[:2])\n",
    "    print( trg_texts[:2])\n",
    "    \n",
    "    print('-----Tokenizing-----')\n",
    "    # truncation = True: max_len 보다 길면 끊고 <eos> 를 추가한다.\n",
    "    # pt: pytorch tensor로 변환\n",
    "    src = tokenizer(src_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt').input_ids\n",
    "    # target 의 첫 입력 단어로 <sos> 를 추가한다.\n",
    "    trg_texts = ['</s> ' + s for s in trg_texts]\n",
    "    trg = tokenizer(trg_texts, padding=True, truncation=True, max_length = max_len, return_tensors='pt').input_ids\n",
    "    \n",
    "    # Batch 의 내 포함된 문장의 최대 길이보다 \n",
    "    # 짧은 문장은 <pad> 로 채워지게 된다.\n",
    "    print(src.shape, trg.shape)\n",
    "    print(src[:2])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
