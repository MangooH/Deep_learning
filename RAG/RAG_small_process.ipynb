{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "> Retriever-Augmented Generation\n",
    "\n",
    ": LLM 이 응답을 생성할 때 학습 데이터 소스 외부의 신뢰할 수 있는 지식 베이스를 참조할 수 있도록 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LangSmith Configure\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, JSONLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.callbacks.manager import CallbackManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain\n",
    "\n",
    ": Langchain은 통합된 인터페이스를 통해 언어 모델, 프롬프트, 외부 데이터 소스 등을 연결하여 복잡한 자연어 처리 태스크를 수행하도록 하는 Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 전처리\n",
    "\n",
    "> 통합형 Loader 인터페이스\n",
    "\n",
    "- 다양한 loader 도구를 활용하여 loader 객체를 생성한다.\n",
    "    - Web page\n",
    "    - pdf\n",
    "    - json \n",
    "    - csv\n",
    "    - etc\n",
    "- 생성된 loader 객체의 `load()` 함수는 전체 문서를 로드하여 반환한다.\n",
    "- 생성된 loader 객체의 `load_and_split()` 함수는 문서를 페이지로 쪼개어 반환한 결과이다.\n",
    "- 반환된 document 구조는 `page_content` 와 `metadata` 속성 값을 포함한다.\n",
    "    - `page_content`는 문서의 내용\n",
    "    - `metadata`는 파일명, 페이지 번호, 그 밖에 유용한 메타정보를 포함한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다양한 문서를 불러오는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n",
      "[Document(page_content=\"\\n  <앵커> \\n  \\n 한 지방교육청이 박람회에 쓸 주제가를 공모했습니다.\\xa0그런데, 1등으로 뽑은 곡이 알고 보니 AI가 만든 노래였는데요, 이 공모전에 심사위원으로 참여했던 유명한 작곡가도 이런 사실을 '전혀 몰랐다'고 말하기도 했습니다. \\n  \\n 홍영재 기자가 보도합니다. \\n  \\n <기자> \\n  \\n 다음 달 전남 여수에서 열리는 '글로컬 미래교육 박람회' 주제가로 선정된 곡입니다. \\n  \\n [제목 '세상에 소리쳐! 글로컬!' : 변화의 바람 우리의 힘을 깨우네. 글로컬 교육이여 미래를 여는 열쇠 가자~! 내가 원하는 미래 글로컬 네가 원하는 미래 글로컬] \\n  \\n 응모작 12곡 가운데 희망찬 가사, 밝은 악풍이 높게 평가돼 한 초등학교 교사가 만든 이 노래가 주제가로 선정됐습니다. \\n  \\n 선정 사실을 통보하는 과정에서 새로운 사실이 드러났습니다. \\n  \\n [전남교육청 관계자 : 이거를 살펴보는 과정에서 선생님께서도 AI 곡이라고 이제 말씀을 주셔 가지고 그래서 나중에 전달이 됐습니다.] \\n  \\n AI 서비스에 문자 명령 입력을 수차례 거쳐 만들었다는 겁니다. \\n  \\n 작곡도, 노래도 모두 AI였던 겁니다. \\n  \\n 심사위원으로 참여한 유명 작곡가 김형석 씨는 '제법 수작이었다'며 당혹했습니다. \\n  \\n [김형석/작곡가 : 전혀 몰랐어요. 전혀 몰랐고. '어 잠깐만 그럼 이걸 1위로 줘야 되나?' 이제 그런 퀘스천 마크(물음표)가 생겼고….] \\n  \\n 주최 측은 AI를 사용하지 말라는 조건이 없었고, 미래세대를 위한 박람회의 주제와 부합하다며 주제곡으로 최종 선정했습니다. \\n  \\n AI가 일상뿐 아니라 예술계 전반으로 확산하면서 창작의 정의에 대한 논란은 거셉니다. \\n  \\n 독일의 한 사진작가는 국제사진전에 일부러 AI가 만든 이미지로 응모한 뒤 1위에 선정되자 수상을 거부하기도 했습니다. \\n  \\n 그림, 음악 등 여러 예술 분야에 이미 AI가 깊숙이 침투했기 때문에 무조건 막을 수는 없다는 의견이 많습니다. \\n  \\n [김형석/작곡가 : 마르셀 뒤샹이 변기통 갖다 놓고 사인해서 엄청난 현대 미술사에 획을 긋는 어떤 그런 행위를 하고 이런 것처럼 AI에게 (명령을 입력하는) 이 행위를 하기 위해서 어떤 생각과 철학과 사상과 혹은 아이덴티티(정체성)에 혹은 인생을 살아왔는지 이런 것들이 훨씬 중요해지지 않을까….] \\n  \\n 예술, 그리고 창작에 대한 새로운 정의, 또 구체적인 기준에 대한 합의라는 숙제가 놓였습니다. \\n  \\n (영상취재 : 한일상·배문산, 영상편집 : 이상민, 디자인 : 김정은)\\n \", metadata={'source': 'https://news.sbs.co.kr/news/endPage.do?news_id=N1007601801&plink=TOTAL&cooper=SBSNEWSSEARCH'})]\n"
     ]
    }
   ],
   "source": [
    "# WebBaseLoader\n",
    "web_loader = WebBaseLoader(\n",
    "    web_path=(\"https://news.sbs.co.kr/news/endPage.do?news_id=N1007601801&plink=TOTAL&cooper=SBSNEWSSEARCH\",),\n",
    "    bs_kwargs= dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            'div',\n",
    "            attrs={\"class\": [\"text_area\",]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = web_loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 16\n",
      "page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023' metadata={'source': '1706.03762v7.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# PyPDFLoader\n",
    "pdf_loader = PyPDFLoader('1706.03762v7.pdf')\n",
    "split_docs = pdf_loader.load_and_split()\n",
    "print(f\"문서의 수: {len(split_docs)}\")\n",
    "print(split_docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행의 수: 9272\n"
     ]
    }
   ],
   "source": [
    "# CSV\n",
    "csv_loader = CSVLoader(file_path='output.csv')\n",
    "data = csv_loader.load()\n",
    "print(f\"행의 수: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='{\"quiz\": {\"sport\": {\"q1\": {\"question\": \"Which one is correct team name in NBA?\", \"options\": [\"New York Bulls\", \"Los Angeles Kings\", \"Golden State Warriros\", \"Huston Rocket\"], \"answer\": \"Huston Rocket\"}}, \"maths\": {\"q1\": {\"question\": \"5 + 7 = ?\", \"options\": [\"10\", \"11\", \"12\", \"13\"], \"answer\": \"12\"}, \"q2\": {\"question\": \"12 - 8 = ?\", \"options\": [\"1\", \"2\", \"3\", \"4\"], \"answer\": \"4\"}}}}', metadata={'source': '/root/rag_prac/example_2.json', 'seq_num': 1})]\n"
     ]
    }
   ],
   "source": [
    "# JSON\n",
    "json_loader = JSONLoader(\n",
    "    file_path='example_2.json',\n",
    "    jq_schema=\".\",\n",
    "    text_content=False,\n",
    ")\n",
    "data = json_loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Interface 전체 Process\n",
    "- `Indexing`: load, split embed, store\n",
    "- `RAG`: prompt, retriever, generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://news.sbs.co.kr/news/endPage.do?news_id=N1007601801&plink=TOTAL&cooper=SBSNEWSSEARCH\",),\n",
    "    bs_kwargs= dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            'div',\n",
    "            attrs={\"class\": [\"text_area\",]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_spliter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, chunk_overlap=50,\n",
    ")\n",
    "\n",
    "splits = text_spliter.split_documents(docs)\n",
    "\n",
    "model_name = \"jhgan/ko-sroberta-multitask\"\n",
    "EMBEDDING = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=splits, embedding=EMBEDDING\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model='yanolja-10.8B:latest',\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최근 김형석 아티스트는 AI가 만든 곡을 한 지방교육청에서 개최한 주제가 공모전의 1위로 선정했습니다. 심사위원인 김형석은 AI 사용 사실을 몰라서 놀랐으며, 이는 예술과 창작의 정의에 대한 질문을 제기하였습니다. 주최 측은 미래 세대를 위한 박람회의 주제를 반영하기 위해 이 노래를 최종적으로 채택했습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'최근 김형석 아티스트는 AI가 만든 곡을 한 지방교육청에서 개최한 주제가 공모전의 1위로 선정했습니다. 심사위원인 김형석은 AI 사용 사실을 몰라서 놀랐으며, 이는 예술과 창작의 정의에 대한 질문을 제기하였습니다. 주최 측은 미래 세대를 위한 박람회의 주제를 반영하기 위해 이 노래를 최종적으로 채택했습니다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"최근 김형석 아티스트가 AI 로 만든 곡을 1위로 뽑았다는데, 자세히 이야기해줘.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
